# -*- coding: utf-8 -*-
"""blackcoffer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12ok1U-TAlhlU9KE37RHuAhD8gInpHF_b
"""

pip install requests beautifulsoup4 selenium nltk textblob pandas openpyxl

nltk.download('all')

import os
import re
import nltk
import pandas as pd
import requests
from bs4 import BeautifulSoup
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords

# ✅ Step 1: Download Necessary NLTK Resources
nltk.download('punkt')
nltk.download('stopwords')

# ✅ Step 2: Read Input File (Ensure Input.xlsx is Uploaded)
input_file = "Input.xlsx"
df = pd.read_excel(input_file)

# ✅ Step 3: Load Stopwords & Sentiment Dictionaries
stop_words = set(stopwords.words('english'))

# Load Positive & Negative Word Lists
def load_word_list(filename):
    with open(filename, "r", encoding="latin-1") as file:
        return set(word.strip().lower() for word in file.readlines())

positive_words = load_word_list("/content/positive-words.txt")
negative_words = load_word_list("/content/negative-words.txt")

# ✅ Step 4: Function to Extract Article Text from URL
def extract_text(url):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.content, "html.parser")

        # Extract Title
        title = soup.title.text if soup.title else ""

        # Extract Main Content (Modify Based on Website Structure)
        paragraphs = soup.find_all("p")
        article_text = " ".join([p.text for p in paragraphs])

        return f"{title}\n{article_text}"
    except Exception as e:
        return ""

# ✅ Step 5: Function for Text Analysis
def analyze_text(text):
    words = word_tokenize(text.lower())  # Tokenize words
    sentences = sent_tokenize(text)      # Tokenize sentences

    # Remove Stopwords & Non-Alphabetic Tokens
    words_filtered = [word for word in words if word.isalpha() and word not in stop_words]

    # Sentiment Scores
    positive_score = sum(1 for word in words_filtered if word in positive_words)
    negative_score = sum(1 for word in words_filtered if word in negative_words)

    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)
    subjectivity_score = (positive_score + negative_score) / (len(words_filtered) + 0.000001)

    # Readability Metrics
    avg_sentence_length = len(words_filtered) / (len(sentences) + 0.000001)
    complex_words = [word for word in words_filtered if len(word) > 6]
    percentage_complex_words = len(complex_words) / (len(words_filtered) + 0.000001)
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    # Word Analysis
    word_count = len(words_filtered)
    syllable_per_word = sum(len(re.findall(r'[aeiouy]+', word)) for word in words_filtered) / (len(words_filtered) + 0.000001)
    personal_pronouns = len(re.findall(r'\b(i|we|my|ours|us)\b', text, re.I))
    avg_word_length = sum(len(word) for word in words_filtered) / (len(words_filtered) + 0.000001)

    return [
        positive_score, negative_score, polarity_score, subjectivity_score,
        avg_sentence_length, percentage_complex_words, fog_index,
        avg_sentence_length, len(complex_words), word_count,
        syllable_per_word, personal_pronouns, avg_word_length
    ]

# ✅ Step 6: Process Each URL in Input.xlsx
output_data = []

for index, row in df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    print(f"Processing {url_id}: {url}")

    # Extract Text & Save to .txt File
    article_text = extract_text(url)
    if article_text:
        with open(f"{url_id}.txt", "w", encoding="utf-8") as f:
            f.write(article_text)

        # Perform Text Analysis
        text_metrics = analyze_text(article_text)
        output_data.append([url_id, url] + text_metrics)
    else:
        print(f"❌ Skipping {url_id} (Failed to fetch content)")

# ✅ Step 7: Save Output Data to Output.xlsx
output_columns = [
    "URL_ID", "URL", "POSITIVE SCORE", "NEGATIVE SCORE", "POLARITY SCORE",
    "SUBJECTIVITY SCORE", "AVG SENTENCE LENGTH", "PERCENTAGE OF COMPLEX WORDS",
    "FOG INDEX", "AVG NUMBER OF WORDS PER SENTENCE", "COMPLEX WORD COUNT",
    "WORD COUNT", "SYLLABLE PER WORD", "PERSONAL PRONOUNS", "AVG WORD LENGTH"
]

output_df = pd.DataFrame(output_data, columns=output_columns)
output_df.to_excel("Output.xlsx", index=False)

print("✅ Analysis complete! Results saved in Output.xlsx")